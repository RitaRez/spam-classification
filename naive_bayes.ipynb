{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88bc4c13",
   "metadata": {},
   "source": [
    "# Classifying spam in emails using Naive Bayes Methods\n",
    "\n",
    "\n",
    "The Naive Bayes' Theorem finds the probability of an event occurring given the probability of another event that has already occurred.\n",
    "\n",
    "According to the Scikit-Learn documentation, \n",
    "\n",
    "> Naive Bayes methods are a set of supervised learning algorithms based on applying Bayesâ€™ theorem with the \"naive\" assumption of conditional independence between every pair of features given the value of the class variable. \n",
    "\n",
    "So given a class variable *Y* and a dependent feature vector *X*<sub>1</sub>... *X*<sub>n</sub>:\n",
    "\n",
    "![image.png](./images/bayes_theorem.png)\n",
    "\n",
    "Assuming conditional independence such that:\n",
    "\n",
    "![image.png](./images/assumption.png)\n",
    "\n",
    "For all *i* this relationship is simplified to\n",
    "\n",
    "![image.png](./images/simplification.png)\n",
    "\n",
    "Since *P(X*<sub>1</sub>,..., *X*<sub>N</sub>*)* is constant for each input, the following classification rule is valid:\n",
    "\n",
    "![image.png](./images/classification.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adbd8aa",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "For this part of the project, several python modules we're needed. **Pandas** was used for handling the data frames, **Seaborn** was used to improve report visualization, **scikit-learn** was used to extract features and for its classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9a20ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16c619d",
   "metadata": {},
   "source": [
    "## Our Dataset\n",
    "For this assignment, I used the [Spam Mails Dataset](https://www.kaggle.com/venky73/spam-mails-dataset) found on kaggle, which was retrieved from [Enron Spam](http://www2.aueb.gr/users/ion/data/enron-spam/). The data frame consists of several e-mail labeled 'ham' (legit emails) and spam. It can be observed bellow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e74a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/spam_ham.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e59ee8",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "As seen bellow there are way more legit emails then spam, to handle with the imbalanced dataset we're going to resample it so to use it in the *Multinomial Naive Bayes* and we're also going to use the *Complement Naive Bayes*, which is particularly suited for imbalanced data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f868d70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spam = df[df['label']=='spam']\n",
    "df_ham = df[df['label']=='ham']\n",
    "\n",
    "print('Amount of hams: ', len(df_ham),' Amount of spams: ', len(df_spam))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0c1517",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes\n",
    "\n",
    "### Resampling data\n",
    "Throught the function *resample* in the **scikit-learn** module, observations in the larger class (hams) will be randomly removed to prevent its signal from dominating the learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5855a7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ham_downsampled = resample(df_ham, replace=False, n_samples=len(df_spam), random_state=42)\n",
    "resampled_df = pd.concat([df_ham_downsampled, df_spam])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa371221",
   "metadata": {},
   "source": [
    "### Train and Test Split \n",
    "\n",
    "Through the function *train_test_split* in the **scikit-learn** module, we will split the data frame in proportions 20-80."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abe6114",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(resampled_df['text'], resampled_df['label_num'], test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4857a7d",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "We are going to use the function *TfidfVectorizer* from the **scikit-learn** module to extract features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba798b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "features_train_transformed = vectorizer.fit_transform(x_train.tolist()) \n",
    "features_test_transformed  = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e30bea",
   "metadata": {},
   "source": [
    "### Train our Naive Bayes Model\n",
    "\n",
    "We'll fit the **scikit-learning** *Naive Bayes* odel, the *MultinomialNB* function, to our *TF-IDF* vector version of x_train, and the true output labels stored in y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2e7673",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = MultinomialNB()\n",
    "classifier.fit(features_train_transformed, y_train)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f669c32",
   "metadata": {},
   "source": [
    "### Measuring Accuracy \n",
    "\n",
    "Finally, using our test sets we'll evalue the model accuracy using built-in functions from the **scikit** library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfaae2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = classifier.predict(features_test_transformed)\n",
    "actual = y_test.tolist()\n",
    "score_2 = f1_score(actual, labels, average = 'binary')\n",
    "\n",
    "print ('Accuracy Score :',accuracy_score(actual, labels))\n",
    "print('F-Measure: %.3f' % score_2)\n",
    "\n",
    "\n",
    "clf_report = classification_report(actual, labels, output_dict=True)\n",
    "sns.heatmap(pd.DataFrame(clf_report).iloc[:-1, :].T, annot=True, cmap=\"GnBu\", vmin=0.70, vmax=1)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84899455",
   "metadata": {},
   "source": [
    "## Complement Naive Bayes\n",
    "\n",
    "### Train and Test Split \n",
    "\n",
    "Through the function *train_test_split* in the **scikit-learn** module, we will split the data frame in proportions 20-80."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f78ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df['text'], df['label_num'], test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bb47e6",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "We are going to use the function *TfidfVectorizer* from the **scikit-learn** module to extract features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19c6039",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_list = x_train.tolist()\n",
    "vectorizer = TfidfVectorizer()\n",
    "features_train_transformed = vectorizer.fit_transform(x_train_list) #gives tf idf vector for x_train\n",
    "features_test_transformed  = vectorizer.transform(x_test) #gives tf idf vector for x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04673d8",
   "metadata": {},
   "source": [
    "### Train our Naive Bayes Model\n",
    "\n",
    "We'll fit the **scikit-learning** *Naive Bayes* odel, the *MultinomialNB* function, to our *TF-IDF* vector version of x_train, and the true output labels stored in y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe4b402",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = ComplementNB()\n",
    "classifier.fit(features_train_transformed, y_train)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85001b5d",
   "metadata": {},
   "source": [
    "### Measuring Accuracy \n",
    "\n",
    "Finally, using our test sets we'll evalue the model accuracy using built-in functions from the **scikit** library.### Measuring Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fca50f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = classifier.predict(features_test_transformed)\n",
    "actual = y_test.tolist()\n",
    "score_2 = f1_score(actual, labels, average = 'binary')\n",
    "\n",
    "print ('Accuracy Score :',accuracy_score(actual, labels))\n",
    "print('F-Measure: %.3f' % score_2)\n",
    "\n",
    "clf_report = classification_report(actual, labels, output_dict=True)\n",
    "sns.heatmap(pd.DataFrame(clf_report).iloc[:-1, :].T, annot=True, cmap=\"GnBu\", vmin=0.70, vmax=1)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66d1b94",
   "metadata": {},
   "source": [
    "## A Few Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ee34a7",
   "metadata": {},
   "source": [
    "## References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
